{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://www.tensorflow.org/text/tutorials/word2vec#compile_all_steps_into_one_function",
   "id": "d8cf714a12d506c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:20:48.557707Z",
     "start_time": "2025-12-14T09:20:48.549060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ],
   "id": "1287fd94f888d2aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:28:50.504826Z",
     "start_time": "2025-12-13T02:28:50.484767Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext tensorboard",
   "id": "edeadff3f24b9a8d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:38:00.946368Z",
     "start_time": "2025-12-14T09:38:00.941282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ],
   "id": "d4f175b0ffc97791",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:39:03.393572Z",
     "start_time": "2025-12-13T02:39:03.384848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "len(tokens)"
   ],
   "id": "95d4189e9e6c7c2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:40:19.116191Z",
     "start_time": "2025-12-13T02:40:19.109950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab, index = {}, 1\n",
    "\n",
    "vocab['<pad>'] = 0\n",
    "for token in tokens:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ],
   "id": "bf61558b2a390730",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:40:49.850885Z",
     "start_time": "2025-12-13T02:40:49.845829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "inverse_vocab"
   ],
   "id": "f08836d701228f8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: 'the',\n",
       " 2: 'wide',\n",
       " 3: 'road',\n",
       " 4: 'shimmered',\n",
       " 5: 'in',\n",
       " 6: 'hot',\n",
       " 7: 'sun'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:41:24.622163Z",
     "start_time": "2025-12-13T02:41:24.617546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ],
   "id": "7953d20a9fd004af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:45:05.963125Z",
     "start_time": "2025-12-13T02:45:05.958736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0,\n",
    "      seed=SEED,)\n",
    "print(len(positive_skip_grams))"
   ],
   "id": "f5f0f70dc27bc284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:48:20.952048Z",
     "start_time": "2025-12-13T02:48:20.943403Z"
    }
   },
   "cell_type": "code",
   "source": "[[inverse_vocab[i], inverse_vocab[j]] for i, j in positive_skip_grams]",
   "id": "2cadc6954c80ee4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in', 'hot'],\n",
       " ['shimmered', 'the'],\n",
       " ['shimmered', 'wide'],\n",
       " ['sun', 'hot'],\n",
       " ['the', 'hot'],\n",
       " ['road', 'wide'],\n",
       " ['road', 'the'],\n",
       " ['shimmered', 'road'],\n",
       " ['in', 'the'],\n",
       " ['hot', 'the'],\n",
       " ['shimmered', 'in'],\n",
       " ['the', 'in'],\n",
       " ['the', 'road'],\n",
       " ['in', 'shimmered'],\n",
       " ['hot', 'sun'],\n",
       " ['in', 'road'],\n",
       " ['wide', 'the'],\n",
       " ['the', 'shimmered'],\n",
       " ['sun', 'the'],\n",
       " ['wide', 'shimmered'],\n",
       " ['hot', 'in'],\n",
       " ['road', 'shimmered'],\n",
       " ['road', 'in'],\n",
       " ['the', 'wide'],\n",
       " ['wide', 'road'],\n",
       " ['the', 'sun']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T02:47:35.352559Z",
     "start_time": "2025-12-13T02:47:35.347096Z"
    }
   },
   "cell_type": "code",
   "source": "positive_skip_grams",
   "id": "8d691feb402c0055",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6],\n",
       " [4, 1],\n",
       " [4, 2],\n",
       " [7, 6],\n",
       " [1, 6],\n",
       " [3, 2],\n",
       " [3, 1],\n",
       " [4, 3],\n",
       " [5, 1],\n",
       " [6, 1],\n",
       " [4, 5],\n",
       " [1, 5],\n",
       " [1, 3],\n",
       " [5, 4],\n",
       " [6, 7],\n",
       " [5, 3],\n",
       " [2, 1],\n",
       " [1, 4],\n",
       " [7, 1],\n",
       " [2, 4],\n",
       " [6, 5],\n",
       " [3, 4],\n",
       " [3, 5],\n",
       " [1, 2],\n",
       " [2, 3],\n",
       " [1, 7]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T03:08:02.535071Z",
     "start_time": "2025-12-13T03:08:02.527240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "targe_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "\n",
    "context_class"
   ],
   "id": "a87d003d80a7ea22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[6]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T03:08:08.996756Z",
     "start_time": "2025-12-13T03:08:08.992356Z"
    }
   },
   "cell_type": "code",
   "source": "context_class.shape",
   "id": "17dbd36f26ae59ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T03:10:58.126033Z",
     "start_time": "2025-12-13T03:10:58.118495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,\n",
    "    num_true=1,\n",
    "    num_sampled=num_ns,\n",
    "    unique=True,\n",
    "    range_max=vocab_size,\n",
    "    seed=SEED,\n",
    "    name=\"negative_sampling_\"\n",
    ")\n",
    "negative_sampling_candidates"
   ],
   "id": "1b0f615fcfff928a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([2, 1, 4, 3])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T03:11:33.104210Z",
     "start_time": "2025-12-13T03:11:33.093695Z"
    }
   },
   "cell_type": "code",
   "source": "[inverse_vocab[index.numpy()] for index in negative_sampling_candidates]",
   "id": "c571eebcdbeb9f9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wide', 'the', 'shimmered', 'road']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:16:40.679334Z",
     "start_time": "2025-12-13T04:16:40.671949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "squeezed_context_class"
   ],
   "id": "42fc559341f91acd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:20:20.385131Z",
     "start_time": "2025-12-13T04:20:20.379427Z"
    }
   },
   "cell_type": "code",
   "source": "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)",
   "id": "a7bd309f296648c9",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:19:37.986090Z",
     "start_time": "2025-12-13T04:19:37.977655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = targe_word"
   ],
   "id": "ab54ec7b56d45fff",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:19:47.915138Z",
     "start_time": "2025-12-13T04:19:47.910479Z"
    }
   },
   "cell_type": "code",
   "source": "target",
   "id": "ced3151edcc132bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:20:23.982199Z",
     "start_time": "2025-12-13T04:20:23.975970Z"
    }
   },
   "cell_type": "code",
   "source": "inverse_vocab[targe_word]",
   "id": "2fdd852686bcb3a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:20:25.958795Z",
     "start_time": "2025-12-13T04:20:25.952885Z"
    }
   },
   "cell_type": "code",
   "source": "context",
   "id": "17020eb630991d7e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([6, 2, 1, 4, 3])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:20:50.574369Z",
     "start_time": "2025-12-13T04:20:50.558476Z"
    }
   },
   "cell_type": "code",
   "source": "[inverse_vocab[c.numpy()] for c in context]",
   "id": "e312955d48e04871",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot', 'wide', 'the', 'shimmered', 'road']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:21:03.507641Z",
     "start_time": "2025-12-13T04:21:03.502672Z"
    }
   },
   "cell_type": "code",
   "source": "label",
   "id": "1a6f0e80709da228",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:21:48.773153Z",
     "start_time": "2025-12-13T04:21:48.764977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[targe_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ],
   "id": "279d5e8eca9c2017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 5\n",
      "target_word     : in\n",
      "context_indices : [6 2 1 4 3]\n",
      "context_words   : ['hot', 'wide', 'the', 'shimmered', 'road']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(target, context, label)",
   "id": "da760a3300d4e8e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T04:23:51.221218Z",
     "start_time": "2025-12-13T04:23:51.215619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ],
   "id": "13e4b6786cda9f66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : 5\n",
      "context : tf.Tensor([6 2 1 4 3], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:27:37.818807Z",
     "start_time": "2025-12-14T09:27:37.808300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(sequence, vocabulary_size=vocab_size, window_size=window_size, negative_samples=0, seed=seed)\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1\n",
    "            )\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\"\n",
    "            )\n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ],
   "id": "666ac64a03f2feb8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:28:52.133065Z",
     "start_time": "2025-12-14T09:28:50.745919Z"
    }
   },
   "cell_type": "code",
   "source": "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')",
   "id": "91e2500112abf4f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001B[1m1115394/1115394\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1us/step\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:30:14.181379Z",
     "start_time": "2025-12-14T09:30:14.160526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "print(lines[:10])"
   ],
   "id": "7e8f410aca35826f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '', 'All:']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:31:47.508595Z",
     "start_time": "2025-12-14T09:31:47.064106Z"
    }
   },
   "cell_type": "code",
   "source": "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))",
   "id": "f37a54091769e632",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 11:31:47.230227: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:34:17.334128Z",
     "start_time": "2025-12-14T09:34:17.326388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')"
   ],
   "id": "98792dbdbcaf26e4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:35:14.578380Z",
     "start_time": "2025-12-14T09:35:14.515345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ],
   "id": "d429b4a68a14ff97",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:35:55.897389Z",
     "start_time": "2025-12-14T09:35:45.245506Z"
    }
   },
   "cell_type": "code",
   "source": "vectorize_layer.adapt(text_ds.batch(1024))",
   "id": "4d3c074215b49faa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 11:35:55.695723: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:36:38.161422Z",
     "start_time": "2025-12-14T09:36:38.141671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ],
   "id": "aa055965fdc4283a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('the'), np.str_('and'), np.str_('to'), np.str_('i'), np.str_('of'), np.str_('you'), np.str_('my'), np.str_('a'), np.str_('that'), np.str_('in'), np.str_('is'), np.str_('not'), np.str_('for'), np.str_('with'), np.str_('me'), np.str_('it'), np.str_('be'), np.str_('your')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:38:07.871200Z",
     "start_time": "2025-12-14T09:38:07.585436Z"
    }
   },
   "cell_type": "code",
   "source": "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()",
   "id": "ee7db1180cf00433",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:39:32.499867Z",
     "start_time": "2025-12-14T09:39:22.958206Z"
    }
   },
   "cell_type": "code",
   "source": "sequences = list(text_vector_ds.as_numpy_iterator())",
   "id": "ad9dd6c262557ec0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 11:39:32.496594: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:39:32.552116Z",
     "start_time": "2025-12-14T09:39:32.547456Z"
    }
   },
   "cell_type": "code",
   "source": "len(sequences)",
   "id": "6370b3f37278800e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32777"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:39:52.141114Z",
     "start_time": "2025-12-14T09:39:52.132509Z"
    }
   },
   "cell_type": "code",
   "source": "sequences[:5]",
   "id": "283e3681cfe8612c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " array([138,  36, 982, 144, 673, 125,  16, 106,   0,   0]),\n",
       " array([34,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " array([106, 106,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " array([ 89, 270,   0,   0,   0,   0,   0,   0,   0,   0])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:42:56.047619Z",
     "start_time": "2025-12-14T09:40:35.462825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences, window_size=2, num_ns=4, vocab_size=vocab_size, seed=SEED\n",
    ")"
   ],
   "id": "f49262c1511c47a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [02:20<00:00, 233.17it/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:43:29.937548Z",
     "start_time": "2025-12-14T09:43:22.459535Z"
    }
   },
   "cell_type": "code",
   "source": "targets, contexts, labels = np.array(targets), np.array(contexts), np.array(labels)",
   "id": "2146bd8dd5b9df32",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:43:31.750479Z",
     "start_time": "2025-12-14T09:43:31.740755Z"
    }
   },
   "cell_type": "code",
   "source": "targets.shape, contexts.shape, labels.shape",
   "id": "1442c9028dbe74f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((620830,), (620830, 5), (620830, 5))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:44:09.076372Z",
     "start_time": "2025-12-14T09:44:09.070901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000"
   ],
   "id": "2e11b6ae441068fc",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:44:15.848736Z",
     "start_time": "2025-12-14T09:44:15.677091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset"
   ],
   "id": "d99df43fda101fea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=((TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(5,), dtype=tf.int64, name=None)), TensorSpec(shape=(5,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:45:05.715094Z",
     "start_time": "2025-12-14T09:45:05.701564Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)",
   "id": "576a214ea1752bce",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:45:11.264865Z",
     "start_time": "2025-12-14T09:45:11.256464Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "ab67166cb32e4b7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:45:45.331544Z",
     "start_time": "2025-12-14T09:45:45.277398Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)",
   "id": "fa361bfcb674c022",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:45:47.954093Z",
     "start_time": "2025-12-14T09:45:47.945461Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "7264ae8e83e8dca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:47:08.158311Z",
     "start_time": "2025-12-14T09:47:04.245336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_PATH = \"saved_word2vec_dataset\"\n",
    "\n",
    "dataset.save(DATASET_PATH)"
   ],
   "id": "b4d511cc2505520",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:47:14.502566Z",
     "start_time": "2025-12-14T09:47:14.346712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_dataset = tf.data.Dataset.load(DATASET_PATH)\n",
    "\n",
    "print(loaded_dataset)\n"
   ],
   "id": "5f975f0ecf374b32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_LoadDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:54:11.483780Z",
     "start_time": "2025-12-14T09:54:11.475626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size, embedding_dim, name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return dots"
   ],
   "id": "20a71a5219a9445f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:55:04.587631Z",
     "start_time": "2025-12-14T09:55:04.582180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ],
   "id": "c494016ac68d54e3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:56:27.396332Z",
     "start_time": "2025-12-14T09:56:27.315505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 64\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ],
   "id": "6232cc2e95263f90",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:56:48.286247Z",
     "start_time": "2025-12-14T09:56:48.280736Z"
    }
   },
   "cell_type": "code",
   "source": "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")",
   "id": "bd2eab8819fab681",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:57:52.063800Z",
     "start_time": "2025-12-14T09:57:00.945859Z"
    }
   },
   "cell_type": "code",
   "source": "word2vec.fit(dataset, epochs=10, callbacks=[tensorboard_callback])",
   "id": "603133b2bddef006",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 11ms/step - accuracy: 0.3314 - loss: 1.4961\n",
      "Epoch 2/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - accuracy: 0.4002 - loss: 1.3934\n",
      "Epoch 3/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.4310 - loss: 1.3433\n",
      "Epoch 4/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 10ms/step - accuracy: 0.4510 - loss: 1.3078\n",
      "Epoch 5/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - accuracy: 0.4660 - loss: 1.2792\n",
      "Epoch 6/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.4790 - loss: 1.2548\n",
      "Epoch 7/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.4901 - loss: 1.2330\n",
      "Epoch 8/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.4997 - loss: 1.2133\n",
      "Epoch 9/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.5084 - loss: 1.1952\n",
      "Epoch 10/10\n",
      "\u001B[1m606/606\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - accuracy: 0.5164 - loss: 1.1785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b5a2645f770>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:57:52.081059Z",
     "start_time": "2025-12-14T09:57:52.075805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ],
   "id": "cd8938b07b359078",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:59:03.378925Z",
     "start_time": "2025-12-14T09:59:03.314434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ],
   "id": "bebde69a4ac4b846",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:59:11.039790Z",
     "start_time": "2025-12-14T09:59:10.082811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "id": "60a99acc3b6c0855",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:59:38.994488Z",
     "start_time": "2025-12-14T09:59:38.986421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ],
   "id": "4f82f91c35df9e4e",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T10:00:11.222963Z",
     "start_time": "2025-12-14T10:00:11.200633Z"
    }
   },
   "cell_type": "code",
   "source": "vocab",
   "id": "41fe622176c20637",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " np.str_('the'),\n",
       " np.str_('and'),\n",
       " np.str_('to'),\n",
       " np.str_('i'),\n",
       " np.str_('of'),\n",
       " np.str_('you'),\n",
       " np.str_('my'),\n",
       " np.str_('a'),\n",
       " np.str_('that'),\n",
       " np.str_('in'),\n",
       " np.str_('is'),\n",
       " np.str_('not'),\n",
       " np.str_('for'),\n",
       " np.str_('with'),\n",
       " np.str_('me'),\n",
       " np.str_('it'),\n",
       " np.str_('be'),\n",
       " np.str_('your'),\n",
       " np.str_('his'),\n",
       " np.str_('this'),\n",
       " np.str_('but'),\n",
       " np.str_('he'),\n",
       " np.str_('have'),\n",
       " np.str_('as'),\n",
       " np.str_('thou'),\n",
       " np.str_('him'),\n",
       " np.str_('so'),\n",
       " np.str_('what'),\n",
       " np.str_('thy'),\n",
       " np.str_('will'),\n",
       " np.str_('no'),\n",
       " np.str_('by'),\n",
       " np.str_('all'),\n",
       " np.str_('king'),\n",
       " np.str_('we'),\n",
       " np.str_('shall'),\n",
       " np.str_('her'),\n",
       " np.str_('if'),\n",
       " np.str_('our'),\n",
       " np.str_('are'),\n",
       " np.str_('do'),\n",
       " np.str_('thee'),\n",
       " np.str_('now'),\n",
       " np.str_('lord'),\n",
       " np.str_('good'),\n",
       " np.str_('on'),\n",
       " np.str_('o'),\n",
       " np.str_('come'),\n",
       " np.str_('from'),\n",
       " np.str_('sir'),\n",
       " np.str_('or'),\n",
       " np.str_('which'),\n",
       " np.str_('more'),\n",
       " np.str_('then'),\n",
       " np.str_('well'),\n",
       " np.str_('at'),\n",
       " np.str_('would'),\n",
       " np.str_('was'),\n",
       " np.str_('they'),\n",
       " np.str_('how'),\n",
       " np.str_('here'),\n",
       " np.str_('she'),\n",
       " np.str_('than'),\n",
       " np.str_('their'),\n",
       " np.str_('them'),\n",
       " np.str_('ill'),\n",
       " np.str_('duke'),\n",
       " np.str_('am'),\n",
       " np.str_('hath'),\n",
       " np.str_('say'),\n",
       " np.str_('let'),\n",
       " np.str_('when'),\n",
       " np.str_('one'),\n",
       " np.str_('go'),\n",
       " np.str_('were'),\n",
       " np.str_('love'),\n",
       " np.str_('may'),\n",
       " np.str_('us'),\n",
       " np.str_('make'),\n",
       " np.str_('upon'),\n",
       " np.str_('yet'),\n",
       " np.str_('richard'),\n",
       " np.str_('like'),\n",
       " np.str_('there'),\n",
       " np.str_('must'),\n",
       " np.str_('should'),\n",
       " np.str_('an'),\n",
       " np.str_('first'),\n",
       " np.str_('why'),\n",
       " np.str_('queen'),\n",
       " np.str_('had'),\n",
       " np.str_('know'),\n",
       " np.str_('man'),\n",
       " np.str_('did'),\n",
       " np.str_('tis'),\n",
       " np.str_('where'),\n",
       " np.str_('see'),\n",
       " np.str_('some'),\n",
       " np.str_('too'),\n",
       " np.str_('death'),\n",
       " np.str_('give'),\n",
       " np.str_('who'),\n",
       " np.str_('these'),\n",
       " np.str_('take'),\n",
       " np.str_('speak'),\n",
       " np.str_('edward'),\n",
       " np.str_('york'),\n",
       " np.str_('mine'),\n",
       " np.str_('such'),\n",
       " np.str_('up'),\n",
       " np.str_('out'),\n",
       " np.str_('henry'),\n",
       " np.str_('romeo'),\n",
       " np.str_('can'),\n",
       " np.str_('father'),\n",
       " np.str_('tell'),\n",
       " np.str_('time'),\n",
       " np.str_('gloucester'),\n",
       " np.str_('most'),\n",
       " np.str_('lady'),\n",
       " np.str_('son'),\n",
       " np.str_('nor'),\n",
       " np.str_('vincentio'),\n",
       " np.str_('hear'),\n",
       " np.str_('life'),\n",
       " np.str_('god'),\n",
       " np.str_('made'),\n",
       " np.str_('art'),\n",
       " np.str_('warwick'),\n",
       " np.str_('think'),\n",
       " np.str_('much'),\n",
       " np.str_('heart'),\n",
       " np.str_('never'),\n",
       " np.str_('doth'),\n",
       " np.str_('brother'),\n",
       " np.str_('ay'),\n",
       " np.str_('before'),\n",
       " np.str_('true'),\n",
       " np.str_('both'),\n",
       " np.str_('thus'),\n",
       " np.str_('cannot'),\n",
       " np.str_('petruchio'),\n",
       " np.str_('any'),\n",
       " np.str_('being'),\n",
       " np.str_('away'),\n",
       " np.str_('blood'),\n",
       " np.str_('name'),\n",
       " np.str_('fair'),\n",
       " np.str_('coriolanus'),\n",
       " np.str_('been'),\n",
       " np.str_('noble'),\n",
       " np.str_('men'),\n",
       " np.str_('menenius'),\n",
       " np.str_('look'),\n",
       " np.str_('again'),\n",
       " np.str_('very'),\n",
       " np.str_('hand'),\n",
       " np.str_('day'),\n",
       " np.str_('pray'),\n",
       " np.str_('own'),\n",
       " np.str_('juliet'),\n",
       " np.str_('done'),\n",
       " np.str_('sweet'),\n",
       " np.str_('second'),\n",
       " np.str_('myself'),\n",
       " np.str_('therefore'),\n",
       " np.str_('leave'),\n",
       " np.str_('great'),\n",
       " np.str_('against'),\n",
       " np.str_('though'),\n",
       " np.str_('poor'),\n",
       " np.str_('honour'),\n",
       " np.str_('down'),\n",
       " np.str_('prince'),\n",
       " np.str_('hast'),\n",
       " np.str_('way'),\n",
       " np.str_('angelo'),\n",
       " np.str_('fear'),\n",
       " np.str_('old'),\n",
       " np.str_('nay'),\n",
       " np.str_('heaven'),\n",
       " np.str_('clarence'),\n",
       " np.str_('till'),\n",
       " np.str_('call'),\n",
       " np.str_('eyes'),\n",
       " np.str_('world'),\n",
       " np.str_('stay'),\n",
       " np.str_('live'),\n",
       " np.str_('stand'),\n",
       " np.str_('nurse'),\n",
       " np.str_('grace'),\n",
       " np.str_('many'),\n",
       " np.str_('comes'),\n",
       " np.str_('ever'),\n",
       " np.str_('even'),\n",
       " np.str_('wife'),\n",
       " np.str_('nothing'),\n",
       " np.str_('iii'),\n",
       " np.str_('die'),\n",
       " np.str_('dead'),\n",
       " np.str_('whose'),\n",
       " np.str_('bear'),\n",
       " np.str_('night'),\n",
       " np.str_('other'),\n",
       " np.str_('isabella'),\n",
       " np.str_('bolingbroke'),\n",
       " np.str_('friends'),\n",
       " np.str_('leontes'),\n",
       " np.str_('head'),\n",
       " np.str_('friar'),\n",
       " np.str_('peace'),\n",
       " np.str_('buckingham'),\n",
       " np.str_('unto'),\n",
       " np.str_('those'),\n",
       " np.str_('better'),\n",
       " np.str_('lords'),\n",
       " np.str_('word'),\n",
       " np.str_('off'),\n",
       " np.str_('gone'),\n",
       " np.str_('tranio'),\n",
       " np.str_('two'),\n",
       " np.str_('mother'),\n",
       " np.str_('hence'),\n",
       " np.str_('marcius'),\n",
       " np.str_('house'),\n",
       " np.str_('still'),\n",
       " np.str_('since'),\n",
       " np.str_('news'),\n",
       " np.str_('lucio'),\n",
       " np.str_('could'),\n",
       " np.str_('sicinius'),\n",
       " np.str_('master'),\n",
       " np.str_('little'),\n",
       " np.str_('gentleman'),\n",
       " np.str_('daughter'),\n",
       " np.str_('soul'),\n",
       " np.str_('thing'),\n",
       " np.str_('put'),\n",
       " np.str_('once'),\n",
       " np.str_('whom'),\n",
       " np.str_('margaret'),\n",
       " np.str_('set'),\n",
       " np.str_('long'),\n",
       " np.str_('himself'),\n",
       " np.str_('face'),\n",
       " np.str_('camillo'),\n",
       " np.str_('words'),\n",
       " np.str_('thine'),\n",
       " np.str_('iv'),\n",
       " np.str_('ere'),\n",
       " np.str_('else'),\n",
       " np.str_('elizabeth'),\n",
       " np.str_('capulet'),\n",
       " np.str_('none'),\n",
       " np.str_('katharina'),\n",
       " np.str_('rest'),\n",
       " np.str_('might'),\n",
       " np.str_('madam'),\n",
       " np.str_('gods'),\n",
       " np.str_('crown'),\n",
       " np.str_('best'),\n",
       " np.str_('young'),\n",
       " np.str_('power'),\n",
       " np.str_('pardon'),\n",
       " np.str_('tongue'),\n",
       " np.str_('dear'),\n",
       " np.str_('part'),\n",
       " np.str_('farewell'),\n",
       " np.str_('citizen'),\n",
       " np.str_('vi'),\n",
       " np.str_('bring'),\n",
       " np.str_('keep'),\n",
       " np.str_('ii'),\n",
       " np.str_('hope'),\n",
       " np.str_('gentle'),\n",
       " np.str_('right'),\n",
       " np.str_('hastings'),\n",
       " np.str_('said'),\n",
       " np.str_('lucentio'),\n",
       " np.str_('find'),\n",
       " np.str_('every'),\n",
       " np.str_('hortensio'),\n",
       " np.str_('forth'),\n",
       " np.str_('came'),\n",
       " np.str_('bid'),\n",
       " np.str_('home'),\n",
       " np.str_('hands'),\n",
       " np.str_('earth'),\n",
       " np.str_('welcome'),\n",
       " np.str_('lets'),\n",
       " np.str_('into'),\n",
       " np.str_('brutus'),\n",
       " np.str_('hold'),\n",
       " np.str_('dost'),\n",
       " np.str_('cause'),\n",
       " np.str_('tears'),\n",
       " np.str_('boy'),\n",
       " np.str_('baptista'),\n",
       " np.str_('back'),\n",
       " np.str_('about'),\n",
       " np.str_('rome'),\n",
       " np.str_('please'),\n",
       " np.str_('thats'),\n",
       " np.str_('provost'),\n",
       " np.str_('mistress'),\n",
       " np.str_('people'),\n",
       " np.str_('makes'),\n",
       " np.str_('help'),\n",
       " np.str_('indeed'),\n",
       " np.str_('heard'),\n",
       " np.str_('full'),\n",
       " np.str_('cousin'),\n",
       " np.str_('thought'),\n",
       " np.str_('show'),\n",
       " np.str_('after'),\n",
       " np.str_('wilt'),\n",
       " np.str_('place'),\n",
       " np.str_('mind'),\n",
       " np.str_('has'),\n",
       " np.str_('grumio'),\n",
       " np.str_('escalus'),\n",
       " np.str_('cominius'),\n",
       " np.str_('pompey'),\n",
       " np.str_('marry'),\n",
       " np.str_('husband'),\n",
       " np.str_('whats'),\n",
       " np.str_('friend'),\n",
       " np.str_('state'),\n",
       " np.str_('shame'),\n",
       " np.str_('mean'),\n",
       " np.str_('within'),\n",
       " np.str_('gremio'),\n",
       " np.str_('while'),\n",
       " np.str_('servant'),\n",
       " np.str_('clifford'),\n",
       " np.str_('only'),\n",
       " np.str_('hither'),\n",
       " np.str_('hes'),\n",
       " np.str_('fathers'),\n",
       " np.str_('gracious'),\n",
       " np.str_('aufidius'),\n",
       " np.str_('royal'),\n",
       " np.str_('rather'),\n",
       " np.str_('prove'),\n",
       " np.str_('northumberland'),\n",
       " np.str_('last'),\n",
       " np.str_('far'),\n",
       " np.str_('eye'),\n",
       " np.str_('duchess'),\n",
       " np.str_('answer'),\n",
       " np.str_('thousand'),\n",
       " np.str_('mercutio'),\n",
       " np.str_('lie'),\n",
       " np.str_('third'),\n",
       " np.str_('another'),\n",
       " np.str_('paulina'),\n",
       " np.str_('meet'),\n",
       " np.str_('claudio'),\n",
       " np.str_('murderer'),\n",
       " np.str_('shalt'),\n",
       " np.str_('lay'),\n",
       " np.str_('child'),\n",
       " np.str_('use'),\n",
       " np.str_('sorrow'),\n",
       " np.str_('lies'),\n",
       " np.str_('joy'),\n",
       " np.str_('comfort'),\n",
       " np.str_('beseech'),\n",
       " np.str_('tomorrow'),\n",
       " np.str_('grief'),\n",
       " np.str_('benvolio'),\n",
       " np.str_('war'),\n",
       " np.str_('sun'),\n",
       " np.str_('polixenes'),\n",
       " np.str_('less'),\n",
       " np.str_('kings'),\n",
       " np.str_('autolycus'),\n",
       " np.str_('years'),\n",
       " np.str_('hour'),\n",
       " np.str_('happy'),\n",
       " np.str_('fortune'),\n",
       " np.str_('fellow'),\n",
       " np.str_('end'),\n",
       " np.str_('thank'),\n",
       " np.str_('montague'),\n",
       " np.str_('holy'),\n",
       " np.str_('business'),\n",
       " np.str_('things'),\n",
       " np.str_('matter'),\n",
       " np.str_('hate'),\n",
       " np.str_('grave'),\n",
       " np.str_('faith'),\n",
       " np.str_('enough'),\n",
       " np.str_('arms'),\n",
       " np.str_('truth'),\n",
       " np.str_('prospero'),\n",
       " np.str_('lives'),\n",
       " np.str_('light'),\n",
       " np.str_('clown'),\n",
       " np.str_('bed'),\n",
       " np.str_('without'),\n",
       " np.str_('three'),\n",
       " np.str_('save'),\n",
       " np.str_('pity'),\n",
       " np.str_('fall'),\n",
       " np.str_('body'),\n",
       " np.str_('nature'),\n",
       " np.str_('kind'),\n",
       " np.str_('get'),\n",
       " np.str_('tybalt'),\n",
       " np.str_('turn'),\n",
       " np.str_('theres'),\n",
       " np.str_('swear'),\n",
       " np.str_('send'),\n",
       " np.str_('laurence'),\n",
       " np.str_('kate'),\n",
       " np.str_('days'),\n",
       " np.str_('saw'),\n",
       " np.str_('hell'),\n",
       " np.str_('fight'),\n",
       " np.str_('false'),\n",
       " np.str_('bianca'),\n",
       " np.str_('believe'),\n",
       " np.str_('ah'),\n",
       " np.str_('law'),\n",
       " np.str_('follow'),\n",
       " np.str_('yourself'),\n",
       " np.str_('yours'),\n",
       " np.str_('villain'),\n",
       " np.str_('uncle'),\n",
       " np.str_('present'),\n",
       " np.str_('means'),\n",
       " np.str_('looks'),\n",
       " np.str_('does'),\n",
       " np.str_('volumnia'),\n",
       " np.str_('talk'),\n",
       " np.str_('sleep'),\n",
       " np.str_('neer'),\n",
       " np.str_('left'),\n",
       " np.str_('foul'),\n",
       " np.str_('bloody'),\n",
       " np.str_('aumerle'),\n",
       " np.str_('anne'),\n",
       " np.str_('city'),\n",
       " np.str_('worthy'),\n",
       " np.str_('woman'),\n",
       " np.str_('told'),\n",
       " np.str_('servingman'),\n",
       " np.str_('proud'),\n",
       " np.str_('need'),\n",
       " np.str_('mercy'),\n",
       " np.str_('land'),\n",
       " np.str_('hearts'),\n",
       " np.str_('brothers'),\n",
       " np.str_('wish'),\n",
       " np.str_('sound'),\n",
       " np.str_('sit'),\n",
       " np.str_('shepherd'),\n",
       " np.str_('sea'),\n",
       " np.str_('play'),\n",
       " np.str_('messenger'),\n",
       " np.str_('together'),\n",
       " np.str_('thyself'),\n",
       " np.str_('sword'),\n",
       " np.str_('justice'),\n",
       " np.str_('high'),\n",
       " np.str_('gaunt'),\n",
       " np.str_('doubt'),\n",
       " np.str_('breath'),\n",
       " np.str_('because'),\n",
       " np.str_('woe'),\n",
       " np.str_('through'),\n",
       " np.str_('majesty'),\n",
       " np.str_('john'),\n",
       " np.str_('horse'),\n",
       " np.str_('fire'),\n",
       " np.str_('either'),\n",
       " np.str_('biondello'),\n",
       " np.str_('under'),\n",
       " np.str_('sovereign'),\n",
       " np.str_('sister'),\n",
       " np.str_('seen'),\n",
       " np.str_('paris'),\n",
       " np.str_('heres'),\n",
       " np.str_('children'),\n",
       " np.str_('catesby'),\n",
       " np.str_('break'),\n",
       " np.str_('maid'),\n",
       " np.str_('heavy'),\n",
       " np.str_('twas'),\n",
       " np.str_('times'),\n",
       " np.str_('signior'),\n",
       " np.str_('lost'),\n",
       " np.str_('gentlemen'),\n",
       " np.str_('content'),\n",
       " np.str_('care'),\n",
       " np.str_('wrong'),\n",
       " np.str_('traitor'),\n",
       " np.str_('says'),\n",
       " np.str_('ready'),\n",
       " np.str_('purpose'),\n",
       " np.str_('oath'),\n",
       " np.str_('norfolk'),\n",
       " np.str_('loves'),\n",
       " np.str_('kill'),\n",
       " np.str_('haste'),\n",
       " np.str_('fly'),\n",
       " np.str_('florizel'),\n",
       " np.str_('edwards'),\n",
       " np.str_('ears'),\n",
       " np.str_('canst'),\n",
       " np.str_('alas'),\n",
       " np.str_('age'),\n",
       " np.str_('weep'),\n",
       " np.str_('warrant'),\n",
       " np.str_('strange'),\n",
       " np.str_('reason'),\n",
       " np.str_('new'),\n",
       " np.str_('liege'),\n",
       " np.str_('late'),\n",
       " np.str_('hermione'),\n",
       " np.str_('grey'),\n",
       " np.str_('each'),\n",
       " np.str_('alone'),\n",
       " np.str_('thoughts'),\n",
       " np.str_('thanks'),\n",
       " np.str_('stands'),\n",
       " np.str_('slain'),\n",
       " np.str_('sent'),\n",
       " np.str_('rivers'),\n",
       " np.str_('remember'),\n",
       " np.str_('kiss'),\n",
       " np.str_('hours'),\n",
       " np.str_('cry'),\n",
       " np.str_('brought'),\n",
       " np.str_('strike'),\n",
       " np.str_('straight'),\n",
       " np.str_('spirit'),\n",
       " np.str_('seem'),\n",
       " np.str_('return'),\n",
       " np.str_('married'),\n",
       " np.str_('mark'),\n",
       " np.str_('loss'),\n",
       " np.str_('lest'),\n",
       " np.str_('knows'),\n",
       " np.str_('having'),\n",
       " np.str_('ground'),\n",
       " np.str_('free'),\n",
       " np.str_('france'),\n",
       " np.str_('didst'),\n",
       " np.str_('charge'),\n",
       " np.str_('souls'),\n",
       " np.str_('service'),\n",
       " np.str_('sebastian'),\n",
       " np.str_('report'),\n",
       " np.str_('person'),\n",
       " np.str_('near'),\n",
       " np.str_('found'),\n",
       " np.str_('cold'),\n",
       " np.str_('born'),\n",
       " np.str_('yea'),\n",
       " np.str_('ye'),\n",
       " np.str_('widow'),\n",
       " np.str_('sin'),\n",
       " np.str_('sight'),\n",
       " np.str_('senator'),\n",
       " np.str_('richmond'),\n",
       " np.str_('lose'),\n",
       " np.str_('heavens'),\n",
       " np.str_('desire'),\n",
       " np.str_('common'),\n",
       " np.str_('yield'),\n",
       " np.str_('twere'),\n",
       " np.str_('tender'),\n",
       " np.str_('sure'),\n",
       " np.str_('soon'),\n",
       " np.str_('shes'),\n",
       " np.str_('itself'),\n",
       " np.str_('general'),\n",
       " np.str_('foot'),\n",
       " np.str_('fool'),\n",
       " np.str_('fie'),\n",
       " np.str_('air'),\n",
       " np.str_('youll'),\n",
       " np.str_('women'),\n",
       " np.str_('saint'),\n",
       " np.str_('prithee'),\n",
       " np.str_('prison'),\n",
       " np.str_('ha'),\n",
       " np.str_('fault'),\n",
       " np.str_('enemy'),\n",
       " np.str_('duty'),\n",
       " np.str_('deep'),\n",
       " np.str_('citizens'),\n",
       " np.str_('arm'),\n",
       " np.str_('unless'),\n",
       " np.str_('twenty'),\n",
       " np.str_('trust'),\n",
       " np.str_('sons'),\n",
       " np.str_('patience'),\n",
       " np.str_('past'),\n",
       " np.str_('mad'),\n",
       " np.str_('known'),\n",
       " np.str_('honest'),\n",
       " np.str_('deed'),\n",
       " np.str_('command'),\n",
       " np.str_('antonio'),\n",
       " np.str_('yes'),\n",
       " np.str_('ten'),\n",
       " np.str_('something'),\n",
       " np.str_('oxford'),\n",
       " np.str_('oer'),\n",
       " np.str_('lewis'),\n",
       " np.str_('heir'),\n",
       " np.str_('fearful'),\n",
       " np.str_('dare'),\n",
       " np.str_('cut'),\n",
       " np.str_('bound'),\n",
       " np.str_('withal'),\n",
       " np.str_('wear'),\n",
       " np.str_('voices'),\n",
       " np.str_('sirrah'),\n",
       " np.str_('serve'),\n",
       " np.str_('revenge'),\n",
       " np.str_('point'),\n",
       " np.str_('open'),\n",
       " np.str_('masters'),\n",
       " np.str_('highness'),\n",
       " np.str_('half'),\n",
       " np.str_('gonzalo'),\n",
       " np.str_('ear'),\n",
       " np.str_('draw'),\n",
       " np.str_('company'),\n",
       " np.str_('beauty'),\n",
       " np.str_('beat'),\n",
       " np.str_('worse'),\n",
       " np.str_('virtue'),\n",
       " np.str_('tale'),\n",
       " np.str_('read'),\n",
       " np.str_('next'),\n",
       " np.str_('neither'),\n",
       " np.str_('marriage'),\n",
       " np.str_('ist'),\n",
       " np.str_('hark'),\n",
       " np.str_('brave'),\n",
       " np.str_('youth'),\n",
       " np.str_('work'),\n",
       " np.str_('side'),\n",
       " np.str_('seek'),\n",
       " np.str_('sake'),\n",
       " np.str_('pleasure'),\n",
       " np.str_('methinks'),\n",
       " np.str_('loving'),\n",
       " np.str_('lancaster'),\n",
       " np.str_('knew'),\n",
       " np.str_('field'),\n",
       " np.str_('enemies'),\n",
       " np.str_('country'),\n",
       " np.str_('counsel'),\n",
       " np.str_('change'),\n",
       " np.str_('become'),\n",
       " np.str_('ask'),\n",
       " np.str_('wars'),\n",
       " np.str_('tower'),\n",
       " np.str_('today'),\n",
       " np.str_('soldiers'),\n",
       " np.str_('pale'),\n",
       " np.str_('miranda'),\n",
       " np.str_('met'),\n",
       " np.str_('given'),\n",
       " np.str_('gates'),\n",
       " np.str_('further'),\n",
       " np.str_('fast'),\n",
       " np.str_('court'),\n",
       " np.str_('coming'),\n",
       " np.str_('behold'),\n",
       " np.str_('took'),\n",
       " np.str_('tonight'),\n",
       " np.str_('themselves'),\n",
       " np.str_('strength'),\n",
       " np.str_('run'),\n",
       " np.str_('rich'),\n",
       " np.str_('peter'),\n",
       " np.str_('merry'),\n",
       " np.str_('lips'),\n",
       " np.str_('ho'),\n",
       " np.str_('gave'),\n",
       " np.str_('elbow'),\n",
       " np.str_('course'),\n",
       " np.str_('confess'),\n",
       " np.str_('tribunes'),\n",
       " np.str_('title'),\n",
       " np.str_('sworn'),\n",
       " np.str_('same'),\n",
       " np.str_('sad'),\n",
       " np.str_('pass'),\n",
       " np.str_('just'),\n",
       " np.str_('curse'),\n",
       " np.str_('between'),\n",
       " np.str_('banishd'),\n",
       " np.str_('already'),\n",
       " np.str_('along'),\n",
       " np.str_('wind'),\n",
       " np.str_('whilst'),\n",
       " np.str_('thomas'),\n",
       " np.str_('spoke'),\n",
       " np.str_('post'),\n",
       " np.str_('pluck'),\n",
       " np.str_('perdita'),\n",
       " np.str_('office'),\n",
       " np.str_('loved'),\n",
       " np.str_('issue'),\n",
       " np.str_('goes'),\n",
       " np.str_('george'),\n",
       " np.str_('england'),\n",
       " np.str_('dream'),\n",
       " np.str_('bosom'),\n",
       " np.str_('bold'),\n",
       " np.str_('bad'),\n",
       " np.str_('wounds'),\n",
       " np.str_('subject'),\n",
       " np.str_('stanley'),\n",
       " np.str_('over'),\n",
       " np.str_('officer'),\n",
       " np.str_('learn'),\n",
       " np.str_('jest'),\n",
       " np.str_('hadst'),\n",
       " np.str_('fetch'),\n",
       " np.str_('thence'),\n",
       " np.str_('suit'),\n",
       " np.str_('speed'),\n",
       " np.str_('soldier'),\n",
       " np.str_('sly'),\n",
       " np.str_('presence'),\n",
       " np.str_('often'),\n",
       " np.str_('green'),\n",
       " np.str_('grant'),\n",
       " np.str_('forget'),\n",
       " np.str_('foe'),\n",
       " np.str_('fit'),\n",
       " np.str_('faults'),\n",
       " np.str_('entreat'),\n",
       " np.str_('dangerous'),\n",
       " np.str_('worth'),\n",
       " np.str_('wit'),\n",
       " np.str_('wert'),\n",
       " np.str_('wast'),\n",
       " np.str_('voice'),\n",
       " np.str_('throne'),\n",
       " np.str_('strong'),\n",
       " np.str_('soft'),\n",
       " np.str_('seems'),\n",
       " np.str_('request'),\n",
       " np.str_('rage'),\n",
       " np.str_('quarrel'),\n",
       " np.str_('prayers'),\n",
       " np.str_('order'),\n",
       " np.str_('mighty'),\n",
       " np.str_('mariana'),\n",
       " np.str_('heads'),\n",
       " np.str_('gold'),\n",
       " np.str_('deny'),\n",
       " np.str_('black'),\n",
       " np.str_('attend'),\n",
       " np.str_('win'),\n",
       " np.str_('wherein'),\n",
       " np.str_('want'),\n",
       " np.str_('throw'),\n",
       " np.str_('thither'),\n",
       " np.str_('somerset'),\n",
       " np.str_('princely'),\n",
       " np.str_('morning'),\n",
       " np.str_('march'),\n",
       " np.str_('mans'),\n",
       " np.str_('lookd'),\n",
       " np.str_('london'),\n",
       " np.str_('lartius'),\n",
       " np.str_('devil'),\n",
       " np.str_('deeds'),\n",
       " np.str_('consent'),\n",
       " np.str_('case'),\n",
       " np.str_('calld'),\n",
       " np.str_('buy'),\n",
       " np.str_('begin'),\n",
       " np.str_('battle'),\n",
       " np.str_('appear'),\n",
       " np.str_('almost'),\n",
       " np.str_('wonder'),\n",
       " np.str_('wise'),\n",
       " np.str_('whence'),\n",
       " np.str_('water'),\n",
       " np.str_('watch'),\n",
       " np.str_('valiant'),\n",
       " np.str_('subjects'),\n",
       " np.str_('speech'),\n",
       " np.str_('ratcliff'),\n",
       " np.str_('pretty'),\n",
       " np.str_('presently'),\n",
       " np.str_('others'),\n",
       " np.str_('music'),\n",
       " np.str_('move'),\n",
       " np.str_('killd'),\n",
       " np.str_('hot'),\n",
       " np.str_('honours'),\n",
       " np.str_('harm'),\n",
       " np.str_('got'),\n",
       " np.str_('feel'),\n",
       " np.str_('earl'),\n",
       " np.str_('consul'),\n",
       " np.str_('brief'),\n",
       " np.str_('breast'),\n",
       " np.str_('ancient'),\n",
       " np.str_('above'),\n",
       " np.str_('whether'),\n",
       " np.str_('towards'),\n",
       " np.str_('tot'),\n",
       " np.str_('t'),\n",
       " np.str_('slave'),\n",
       " np.str_('roman'),\n",
       " np.str_('poison'),\n",
       " np.str_('murder'),\n",
       " np.str_('mowbray'),\n",
       " np.str_('measure'),\n",
       " np.str_('mayor'),\n",
       " np.str_('least'),\n",
       " np.str_('knock'),\n",
       " np.str_('held'),\n",
       " np.str_('hang'),\n",
       " np.str_('forward'),\n",
       " np.str_('five'),\n",
       " np.str_('eer'),\n",
       " np.str_('alack'),\n",
       " np.str_('3'),\n",
       " np.str_('wouldst'),\n",
       " np.str_('witness'),\n",
       " np.str_('virgilia'),\n",
       " np.str_('touch'),\n",
       " np.str_('promise'),\n",
       " np.str_('mortal'),\n",
       " np.str_('moon'),\n",
       " np.str_('living'),\n",
       " np.str_('keeper'),\n",
       " np.str_('hereford'),\n",
       " np.str_('hearing'),\n",
       " np.str_('flesh'),\n",
       " np.str_('em'),\n",
       " np.str_('curtis'),\n",
       " np.str_('close'),\n",
       " np.str_('awhile'),\n",
       " np.str_('ariel'),\n",
       " np.str_('worst'),\n",
       " np.str_('wherefore'),\n",
       " np.str_('weeping'),\n",
       " np.str_('virtuous'),\n",
       " np.str_('toward'),\n",
       " np.str_('quickly'),\n",
       " np.str_('pay'),\n",
       " np.str_('patient'),\n",
       " np.str_('ourselves'),\n",
       " np.str_('letters'),\n",
       " np.str_('lawful'),\n",
       " np.str_('guilty'),\n",
       " np.str_('grow'),\n",
       " np.str_('greater'),\n",
       " np.str_('goodly'),\n",
       " np.str_('dot'),\n",
       " np.str_('deserved'),\n",
       " np.str_('chance'),\n",
       " np.str_('breathe'),\n",
       " np.str_('blows'),\n",
       " np.str_('besides'),\n",
       " np.str_('base'),\n",
       " np.str_('antigonus'),\n",
       " np.str_('anon'),\n",
       " np.str_('worship'),\n",
       " np.str_('woes'),\n",
       " np.str_('whither'),\n",
       " np.str_('truly'),\n",
       " np.str_('tear'),\n",
       " np.str_('stir'),\n",
       " np.str_('small'),\n",
       " np.str_('shows'),\n",
       " np.str_('pedant'),\n",
       " np.str_('ont'),\n",
       " np.str_('offence'),\n",
       " np.str_('note'),\n",
       " np.str_('needs'),\n",
       " np.str_('mouth'),\n",
       " np.str_('longer'),\n",
       " np.str_('ladies'),\n",
       " np.str_('henrys'),\n",
       " np.str_('golden'),\n",
       " np.str_('fled'),\n",
       " np.str_('feast'),\n",
       " np.str_('exeter'),\n",
       " np.str_('englands'),\n",
       " np.str_('derby'),\n",
       " np.str_('daughters'),\n",
       " np.str_('beg'),\n",
       " np.str_('banishment'),\n",
       " np.str_('angry'),\n",
       " np.str_('year'),\n",
       " np.str_('won'),\n",
       " np.str_('until'),\n",
       " np.str_('traitors'),\n",
       " np.str_('town'),\n",
       " np.str_('taen'),\n",
       " np.str_('sudden'),\n",
       " np.str_('speaks'),\n",
       " np.str_('slew'),\n",
       " np.str_('short'),\n",
       " np.str_('shed'),\n",
       " np.str_('sense'),\n",
       " np.str_('rough'),\n",
       " np.str_('queens'),\n",
       " np.str_('plain'),\n",
       " np.str_('padua'),\n",
       " np.str_('liberty'),\n",
       " np.str_('lands'),\n",
       " np.str_('laid'),\n",
       " np.str_('knee'),\n",
       " np.str_('isabel'),\n",
       " np.str_('hard'),\n",
       " np.str_('enter'),\n",
       " np.str_('doing'),\n",
       " np.str_('defend'),\n",
       " np.str_('clouds'),\n",
       " np.str_('choose'),\n",
       " np.str_('book'),\n",
       " np.str_('bohemia'),\n",
       " np.str_('behind'),\n",
       " np.str_('bears'),\n",
       " np.str_('aside'),\n",
       " np.str_('amen'),\n",
       " np.str_('action'),\n",
       " np.str_('vow'),\n",
       " np.str_('visit'),\n",
       " np.str_('victory'),\n",
       " np.str_('stood'),\n",
       " np.str_('prisoner'),\n",
       " np.str_('princes'),\n",
       " np.str_('plantagenet'),\n",
       " np.str_('piece'),\n",
       " np.str_('oft'),\n",
       " np.str_('noise'),\n",
       " np.str_('neck'),\n",
       " np.str_('mothers'),\n",
       " np.str_('lived'),\n",
       " np.str_('leisure'),\n",
       " np.str_('lead'),\n",
       " np.str_('kingdom'),\n",
       " np.str_('int'),\n",
       " np.str_('hundred'),\n",
       " np.str_('hide'),\n",
       " np.str_('four'),\n",
       " np.str_('fine'),\n",
       " np.str_('dry'),\n",
       " np.str_('deliver'),\n",
       " np.str_('conscience'),\n",
       " np.str_('commend'),\n",
       " np.str_('certain'),\n",
       " np.str_('broke'),\n",
       " np.str_('bride'),\n",
       " np.str_('bones'),\n",
       " np.str_('blow'),\n",
       " np.str_('bawd'),\n",
       " np.str_('act'),\n",
       " np.str_('yourselves'),\n",
       " np.str_('xi'),\n",
       " np.str_('womb'),\n",
       " np.str_('wheres'),\n",
       " np.str_('watchman'),\n",
       " np.str_('wash'),\n",
       " np.str_('twixt'),\n",
       " np.str_('twice'),\n",
       " np.str_('teach'),\n",
       " np.str_('shake'),\n",
       " np.str_('seat'),\n",
       " np.str_('rutland'),\n",
       " np.str_('rise'),\n",
       " np.str_('proceed'),\n",
       " np.str_('pride'),\n",
       " np.str_('morrow'),\n",
       " np.str_('letter'),\n",
       " np.str_('knowledge'),\n",
       " np.str_('knees'),\n",
       " np.str_('kept'),\n",
       " np.str_('herself'),\n",
       " np.str_('glory'),\n",
       " np.str_('glad'),\n",
       " np.str_('fortunes'),\n",
       " np.str_('forgot'),\n",
       " np.str_('forbid'),\n",
       " np.str_('few'),\n",
       " np.str_('drink'),\n",
       " np.str_('danger'),\n",
       " np.str_('countrys'),\n",
       " np.str_('chamber'),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a183dc719feb07e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
