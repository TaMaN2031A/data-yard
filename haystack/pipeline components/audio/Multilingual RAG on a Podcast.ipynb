{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:06:56.277240Z",
     "start_time": "2025-12-12T18:06:45.370517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from haystack.components.audio import LocalWhisperTranscriber\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "whisper = LocalWhisperTranscriber(model=\"tiny\",)\n",
    "whisper.warm_up()\n",
    "transcription = whisper.run(sources=[\"podcast.mp3\"])\n",
    "\n",
    "with open('podcast_transcript_whisper_small.txt.txt', 'w') as fo:\n",
    "  fo.write(transcription[\"documents\"][0].content)\n"
   ],
   "id": "3a29e767cd8b6390",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyad/haystack/hastack_venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:21:35.531868Z",
     "start_time": "2025-12-12T18:21:31.619493Z"
    }
   },
   "cell_type": "code",
   "source": "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
   "id": "669fcd29b6578953",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyad/haystack/hastack_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:25:25.925522Z",
     "start_time": "2025-12-12T18:25:12.061974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder, SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "document_store = QdrantDocumentStore(\n",
    "    \":memory:\",\n",
    "    embedding_dim=1024\n",
    ")"
   ],
   "id": "de4025fedaec50c2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:29:24.385623Z",
     "start_time": "2025-12-12T18:29:24.377468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pp = Pipeline()\n",
    "pp.add_component('text_file_converter', TextFileToDocument())\n",
    "pp.add_component('splitter', DocumentSplitter(split_by='word', split_length=200))\n",
    "pp.add_component('embedder', SentenceTransformersDocumentEmbedder(\n",
    "        model=\"intfloat/multilingual-e5-large\",  # good multilingual model: https://huggingface.co/intfloat/multilingual-e5-large\n",
    "        device=ComponentDevice.from_str(\"cuda:0\"),    # load the model on GPU\n",
    "        prefix=\"passage:\",  # as explained in the model card (https://huggingface.co/intfloat/multilingual-e5-large#faq), documents should be prefixed with \"passage:\"\n",
    "    ))\n",
    "pp.add_component('writer', DocumentWriter(document_store=document_store))"
   ],
   "id": "6cfa02da75dfdd43",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "p.remove_component('')",
   "id": "13a7396af1b8bf70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:29:40.583429Z",
     "start_time": "2025-12-12T18:29:40.569730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pp.connect(\"text_file_converter\", \"splitter\")\n",
    "pp.connect(\"splitter\", \"embedder\")\n",
    "pp.connect(\"embedder\", \"writer\")"
   ],
   "id": "172a83b68db5ad97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x70c4430d9910>\n",
       "ðŸš… Components\n",
       "  - text_file_converter: TextFileToDocument\n",
       "  - splitter: DocumentSplitter\n",
       "  - embedder: SentenceTransformersDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_file_converter.documents -> splitter.documents (list[Document])\n",
       "  - splitter.documents -> embedder.documents (list[Document])\n",
       "  - embedder.documents -> writer.documents (list[Document])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:44:21.291214Z",
     "start_time": "2025-12-12T18:30:13.041996Z"
    }
   },
   "cell_type": "code",
   "source": "res = pp.run({\"text_file_converter\":{\"sources\":[\"podcast_transcript_whisper_small.txt\"]}})",
   "id": "4fe09235f001ab09",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m res = \u001B[43mpp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtext_file_converter\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msources\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpodcast_transcript_whisper_small.txt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/haystack/core/pipeline/pipeline.py:221\u001B[39m, in \u001B[36mPipeline.run\u001B[39m\u001B[34m(self, data, include_outputs_from, break_point, pipeline_snapshot)\u001B[39m\n\u001B[32m    217\u001B[39m     _validate_break_point_against_pipeline(break_point, \u001B[38;5;28mself\u001B[39m.graph)\n\u001B[32m    219\u001B[39m \u001B[38;5;66;03m# TODO: Remove this warmup once we can check reliably whether a component has been warmed up or not\u001B[39;00m\n\u001B[32m    220\u001B[39m \u001B[38;5;66;03m# As of now it's here to make sure we don't have failing tests that assume warm_up() is called in run()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m221\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mwarm_up\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m include_outputs_from \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    224\u001B[39m     include_outputs_from = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/haystack/core/pipeline/base.py:834\u001B[39m, in \u001B[36mPipelineBase.warm_up\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.graph.nodes[node][\u001B[33m\"\u001B[39m\u001B[33minstance\u001B[39m\u001B[33m\"\u001B[39m], \u001B[33m\"\u001B[39m\u001B[33mwarm_up\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    833\u001B[39m     logger.info(\u001B[33m\"\u001B[39m\u001B[33mWarming up component \u001B[39m\u001B[38;5;132;01m{node}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m, node=node)\n\u001B[32m--> \u001B[39m\u001B[32m834\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnodes\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minstance\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwarm_up\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/haystack/components/embedders/sentence_transformers_document_embedder.py:213\u001B[39m, in \u001B[36mSentenceTransformersDocumentEmbedder.warm_up\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    209\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    210\u001B[39m \u001B[33;03mInitializes the component.\u001B[39;00m\n\u001B[32m    211\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.embedding_backend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     \u001B[38;5;28mself\u001B[39m.embedding_backend = \u001B[43m_SentenceTransformersEmbeddingBackendFactory\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_embedding_backend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    215\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_torch_str\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m        \u001B[49m\u001B[43mauth_token\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tokenizer_kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.tokenizer_kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mmodel_max_length\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mself\u001B[39m.embedding_backend.model.max_seq_length = \u001B[38;5;28mself\u001B[39m.tokenizer_kwargs[\u001B[33m\"\u001B[39m\u001B[33mmodel_max_length\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/haystack/components/embedders/backends/sentence_transformers_backend.py:59\u001B[39m, in \u001B[36m_SentenceTransformersEmbeddingBackendFactory.get_embedding_backend\u001B[39m\u001B[34m(model, device, auth_token, trust_remote_code, revision, local_files_only, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, backend)\u001B[39m\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m embedding_backend_id \u001B[38;5;129;01min\u001B[39;00m _SentenceTransformersEmbeddingBackendFactory._instances:\n\u001B[32m     57\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _SentenceTransformersEmbeddingBackendFactory._instances[embedding_backend_id]\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m embedding_backend = \u001B[43m_SentenceTransformersEmbeddingBackend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[43mauth_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     73\u001B[39m _SentenceTransformersEmbeddingBackendFactory._instances[embedding_backend_id] = embedding_backend\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m embedding_backend\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/haystack/components/embedders/backends/sentence_transformers_backend.py:99\u001B[39m, in \u001B[36m_SentenceTransformersEmbeddingBackend.__init__\u001B[39m\u001B[34m(self, model, device, auth_token, trust_remote_code, revision, local_files_only, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, backend)\u001B[39m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(  \u001B[38;5;66;03m# pylint: disable=too-many-positional-arguments\u001B[39;00m\n\u001B[32m     83\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m     84\u001B[39m     *,\n\u001B[32m   (...)\u001B[39m\u001B[32m     95\u001B[39m     backend: Literal[\u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33monnx\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mopenvino\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     96\u001B[39m ):\n\u001B[32m     97\u001B[39m     sentence_transformers_import.check()\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = \u001B[43mSentenceTransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth_token\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresolve_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mauth_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncate_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:367\u001B[39m, in \u001B[36mSentenceTransformer.__init__\u001B[39m\u001B[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001B[39m\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    365\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m367\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[38;5;28mself\u001B[39m.is_hpu_graph_enabled = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    370\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.default_prompt_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.default_prompt_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.prompts:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1368\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1369\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1371\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 930 (1 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    955\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m957\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1350\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1351\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1352\u001B[39m             device,\n\u001B[32m   1353\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1354\u001B[39m             non_blocking,\n\u001B[32m   1355\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1356\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1357\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1360\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1361\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1362\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/haystack/hastack_venv/lib/python3.12/site-packages/torch/cuda/__init__.py:410\u001B[39m, in \u001B[36m_lazy_init\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[32m    406\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    407\u001B[39m     )\n\u001B[32m    408\u001B[39m \u001B[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001B[39;00m\n\u001B[32m    409\u001B[39m \u001B[38;5;66;03m# are found or any other error occurs\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m410\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[32m    412\u001B[39m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[32m    413\u001B[39m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[32m    414\u001B[39m _tls.is_initializing = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack.components.generators.chat import HuggingFaceAPIChatGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack.utils.hf import HFGenerationAPIType\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from pprint import pprint\n",
    "from getpass import getpass"
   ],
   "id": "c1494c43f2bdc730"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your HF API key\")"
   ],
   "id": "d38b22753a376071"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = [\n",
    "    ChatMessage.from_user(\"\"\"\n",
    "    Using only the information contained in these documents in Italian, answer the question using English.\n",
    "If the answer cannot be inferred from the documents, respond \\\"I don't know\\\".\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "    \"\"\")\n",
    "]"
   ],
   "id": "1dc6f3e4f252cd1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the query pipeline\n",
    "query_pipeline = Pipeline()\n",
    "\n",
    "query_pipeline.add_component(\n",
    "    \"text_embedder\",\n",
    "    SentenceTransformersTextEmbedder(\n",
    "        model=\"intfloat/multilingual-e5-large\",  # good multilingual model: https://huggingface.co/intfloat/multilingual-e5-large\n",
    "        device=ComponentDevice.from_str(\"cuda:0\"),  # load the model on GPU\n",
    "        prefix=\"query:\",  # as explained in the model card (https://huggingface.co/intfloat/multilingual-e5-large#faq), queries should be prefixed with \"query:\"\n",
    "    ))\n",
    "query_pipeline.add_component(\"retriever\", QdrantEmbeddingRetriever(document_store=document_store))\n",
    "query_pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(template=template))\n",
    "\n",
    "generator = HuggingFaceAPIChatGenerator(api_type=HFGenerationAPIType.SERVERLESS_INFERENCE_API,\n",
    "                                  api_params={\"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                                             \"provider\": \"together\"},)\n",
    "\n",
    "query_pipeline.add_component(\"generator\", generator)\n",
    "\n",
    "# connect the components\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "query_pipeline.connect(\"prompt_builder\", \"generator\")\n"
   ],
   "id": "6e2c7cf1cf5b1f6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# try the pipeline\n",
    "\n",
    "question = \"What is Pointer Podcast?\"\n",
    "results = query_pipeline.run(\n",
    "    {   \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "for d in results['generator']['replies']:\n",
    "  pprint(d.text)\n"
   ],
   "id": "971a06d3cd06969c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# let's create a simple wrapper to call the pipeline and show the answers\n",
    "\n",
    "def ask_rag(question: str):\n",
    "  results = query_pipeline.run(\n",
    "      {\n",
    "          \"text_embedder\": {\"text\": question},\n",
    "          \"prompt_builder\": {\"question\": question},\n",
    "      }\n",
    "  )\n",
    "\n",
    "  for d in results[\"generator\"][\"replies\"]:\n",
    "      pprint(d.text)\n",
    "import random\n",
    "questions=\"\"\"What are some interesting directions in Large Language Models?\n",
    "What is Haystack?\n",
    "What is Ollama?\n",
    "How did Stefano end up working at deepset?\n",
    "Will open source models achieve the quality of closed ones?\n",
    "What are the main features of Haystack?\n",
    "Summarize in a bulleted list the main stages of training a Large Language Model\n",
    "What is Zephyr?\n",
    "What is it and why is the quantization of Large Language Models interesting?\n",
    "Could you point out the names of the hosts and guests of the podcast?\"\"\".split(\"\\n\")\n",
    "q = random.choice(questions)\n",
    "print(q)\n",
    "ask_rag(q)\n"
   ],
   "id": "4aa018763c8d1bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
